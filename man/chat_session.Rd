% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/class_chat_session.R
\name{chat_session}
\alias{chat_session}
\title{Create Analysis Chat Session}
\usage{
chat_session(
  analysis_type = "fa",
  llm_provider,
  llm_model = NULL,
  system_prompt = NULL,
  params = NULL,
  echo = "none",
  word_limit = 100
)
}
\arguments{
\item{analysis_type}{Character. Type of analysis: "fa" (factor analysis), "gm" (gaussian mixture),
"irt" (item response theory), or "cdm" (cognitive diagnosis model)}

\item{llm_provider}{Character. LLM provider (e.g., "anthropic", "openai", "ollama")}

\item{llm_model}{Character. Model name (e.g., "claude-haiku-4-5-20251001")}

\item{system_prompt}{Character or NULL. Optional custom system prompt text to override the model-specific
default system prompt. Use this to provide institution- or project-specific framing for the LLM
(e.g., preferred terminology, audience level, or reporting conventions). If NULL, the model-specific
default system prompt is used (default = NULL).}

\item{params}{List. ellmer parameters (temperature, etc.). Default uses ellmer::params().
Note: Some providers may not support all parameters (e.g., Ollama doesn't support 'seed').
Unsupported parameters will generate warnings from ellmer but won't affect functionality.}

\item{echo}{Character. Echo level ("none", "output", "all"). Default is "none"}

\item{word_limit}{Integer. Word limit for interpretations (only used if system_prompt is NULL).
Default is 100.}
}
\value{
A chat_session object containing the persistent chat session
}
\description{
Creates a persistent chat session for psychometric interpretation that can be reused
across multiple analyses to avoid repeating the system prompt and reduce
token costs.
}
\details{
The chat_session object stores:
\itemize{
\item A persistent ellmer chat session with the model-specific system prompt already loaded
\item Analysis type, provider, and model information
\item Token usage tracking (cumulative input/output tokens across all interpretations)
\item Session metadata (number of interpretations run, creation timestamp)
}

This allows for efficient reuse across multiple interpretations without
resending the system prompt each time.

\strong{Token Tracking Note:}
The package tracks per-interpretation token counts using roles returned by
\code{ellmer::chat$get_tokens()} (user/assistant). Some providers or the \code{ellmer}
wrapper do not consistently expose \code{system}/\code{system_prompt} token counts (this
appears to be an upstream limitation/bug). To avoid incorrect cumulative
accounting (double-counting or negative accumulation), \code{chat_session} intentionally
does NOT include \code{system_prompt} tokens in the package-level cumulative
counters (\code{total_input_tokens} / \code{total_output_tokens}).

If you require a provider-specific view that includes system prompt tokens,
call \code{chat$chat$get_tokens(include_system_prompt = TRUE)} directly - but note
that results may vary across providers.
}
\examples{
\dontrun{
# Create a persistent chat session for factor analysis
chat <- chat_session("fa", "anthropic", "claude-haiku-4-5-20251001")

# Use with multiple analyses
result1 <- interpret_fa(loadings1, var_info1, chat_session = chat)
result2 <- interpret_fa(loadings2, var_info2, chat_session = chat)

# Check token usage
print(chat)

# Create session for different analysis type
chat_gm <- chat_session("gm", "anthropic", "claude-haiku-4-5-20251001")
}

}
