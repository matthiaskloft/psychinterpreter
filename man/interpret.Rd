% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/interpret_method_dispatch.R
\name{interpret}
\alias{interpret}
\title{Interpret Psychometric Analysis Results}
\usage{
interpret(
  fit_results = NULL,
  variable_info = NULL,
  chat_session = NULL,
  model_type = NULL,
  provider = NULL,
  model = NULL,
  llm_args = NULL,
  fa_args = NULL,
  output_args = NULL,
  ...
)
}
\arguments{
\item{fit_results}{One of:
\itemize{
\item \strong{Fitted model object}:
\itemize{
\item psych package: \code{fa()}, \code{principal()}
\item lavaan package: \code{cfa()}, \code{sem()}, \code{efa()}
\item mirt package: \code{mirt()}
}
\item \strong{Structured list} with model components:
\itemize{
\item For FA: \code{list(loadings = matrix, factor_cor_mat = matrix)} or \code{list(loadings = matrix)}
(both loadings and factor_cor_mat can be matrices or data.frames)
\item For GM: Not yet implemented
\item For IRT: Not yet implemented
\item For CDM: Not yet implemented
}
}}

\item{variable_info}{Dataframe with 'variable' and 'description' columns describing the variables.}

\item{chat_session}{Optional. A chat_session object created with \code{\link{chat_session}}
for token-efficient multi-analysis workflows (default = NULL).}

\item{model_type}{Character. Type of analysis ("fa", "gm", "irt", "cdm"). Required when using
structured list without chat_session. Automatically inferred from chat_session if provided (default = NULL).}

\item{provider}{Character. LLM provider (e.g., "openai", "anthropic", "ollama", "gemini").
Required when chat_session is NULL. Top-level convenience parameter (default = NULL).}

\item{model}{Character. Specific model to use (e.g., "gpt-4o-mini", "claude-3-5-sonnet-20241022", "gemma2:9b").
If NULL, uses provider default. Top-level convenience parameter (default = NULL).}

\item{llm_args}{List or llm_args object. LLM configuration settings. Can be created with
\code{\link{llm_args}} or passed as a plain list. Contains: system_prompt, params, word_limit,
interpretation_guidelines, additional_info, echo. If provider/model are provided at top-level,
they override values in llm_args (default = NULL).}

\item{fa_args}{List or fa_args object. Factor analysis configuration settings. Can be created with
\code{\link{fa_args}} or passed as a plain list. Contains: cutoff, n_emergency, hide_low_loadings,
sort_loadings, factor_cor_mat (default = NULL).}

\item{output_args}{List or output_args object. Output configuration settings. Can be created with
\code{\link{output_args}} or passed as a plain list. Contains: format, heading_level,
suppress_heading, max_line_length, silent (default = NULL).}

\item{...}{Additional arguments passed to model-specific methods.}
}
\value{
Model-specific interpretation object:
\itemize{
\item FA: \code{fa_interpretation} (see \code{\link{interpret}})
\item Future: \code{gm_interpretation}, \code{irt_interpretation}, etc.
}
}
\description{
Unified interface for interpreting psychometric analysis results with LLMs.
Supports fitted model objects, structured lists, and persistent chat sessions
for efficient multi-analysis workflows.
}
\details{
All arguments are named to prevent positional confusion. The function detects which pattern
you're using based on which arguments are provided.
\subsection{Usage Patterns}{

\strong{Pattern 1: Fitted Model Object}

Automatically extracts model components from fitted objects.

\preformatted{
interpret(
  fit_results = fa_model,
  variable_info = var_info,
  provider = "ollama",
  model = "gpt-oss:20b-cloud"
)
}

\strong{Pattern 2: Structured List}

For custom data structures or manual extraction.

For FA, provide loadings (required) and optionally factor_cor_mat:

\preformatted{
interpret(
  fit_results = list(
    loadings = loadings_matrix,
    factor_cor_mat = factor_cor_mat
  ),
  variable_info = var_info,
  model_type = "fa"
)
}

\strong{Pattern 3: Chat Session (Token-Efficient)}

Reuse chat session across analyses to save tokens.

\preformatted{
chat <- chat_session(model_type = "fa", provider = "ollama", model = "gpt-oss:20b-cloud")
result1 <- interpret(chat_session = chat, fit_results = model1, variable_info = var_info1)
result2 <- interpret(chat_session = chat, fit_results = model2, variable_info = var_info2)
}
}

\subsection{Supported Model Types}{
\itemize{
\item \strong{fa} (Factor Analysis): psych::fa(), psych::principal(), lavaan::cfa/efa(), mirt::mirt()
\item \strong{gm} (Gaussian Mixture): Not yet implemented
\item \strong{irt} (Item Response Theory): Not yet implemented
\item \strong{cdm} (Cognitive Diagnosis Models): Not yet implemented
}
}
}
\note{
While only \code{provider} and \code{model} are exposed as top-level parameters for convenience,
the dual interface pattern allows any argument to be passed either directly or through the respective
configuration object (\code{llm_args}, \code{fa_args}, \code{output_args}). This keeps the primary
signature clean while maintaining flexibility
}
\examples{
\dontrun{
library(psych)
fa_model <- fa(mtcars[,1:4], nfactors = 2, rotate = "oblimin")

var_info <- data.frame(
  variable = c("mpg", "cyl", "disp", "hp"),
  description = c("Miles per gallon", "Cylinders", "Displacement", "Horsepower")
)

# Pattern 1: Fitted model
result1 <- interpret(
  fit_results = fa_model,
  variable_info = var_info,
  provider = "ollama",
  model = "gpt-oss:20b-cloud"
)

# Pattern 2: Structured list
# Extract loadings from fitted model
loadings <- as.data.frame(unclass(fa_model$loadings))

# Option A: Loadings only (orthogonal rotation)
result2a <- interpret(
  fit_results = list(loadings = loadings),
  variable_info = var_info,
  model_type = "fa",
  provider = "ollama",
  model = "gpt-oss:20b-cloud"
)

# Option B: Loadings + factor correlations (oblique rotation)
result2b <- interpret(
  fit_results = list(
    loadings = loadings,
    factor_cor_mat = fa_model$Phi
  ),
  variable_info = var_info,
  model_type = "fa",
  provider = "ollama",
  model = "gpt-oss:20b-cloud"
)

# Pattern 3: Chat session (token-efficient for multiple analyses)
chat <- chat_session(model_type = "fa", provider = "ollama", model = "gpt-oss:20b-cloud")
result3a <- interpret(chat_session = chat, fit_results = fa_model, variable_info = var_info)
result3b <- interpret(chat_session = chat, fit_results = fa_model2, variable_info = var_info2)
print(chat)  # Check token usage
}
}
\seealso{
\code{\link{interpret}}, \code{\link{chat_session}}
}
