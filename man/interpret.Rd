% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/core_interpret_dispatch.R
\name{interpret}
\alias{interpret}
\title{Interpret Psychometric Analysis Results}
\usage{
interpret(
  fit_results = NULL,
  chat_session = NULL,
  analysis_type = NULL,
  llm_provider = NULL,
  llm_model = NULL,
  llm_args = NULL,
  interpretation_args = NULL,
  output_args = NULL,
  ...
)
}
\arguments{
\item{fit_results}{One of:
\itemize{
\item \strong{Fitted model object}:
\itemize{
\item psych package: \code{fa()}, \code{principal()}
\item lavaan package: \code{cfa()}, \code{sem()}, \code{efa()}
\item mirt package: \code{mirt()}
}
\item \strong{Structured list} with model components:
\itemize{
\item For FA: \code{list(loadings = matrix, factor_cor_mat = matrix)} or \code{list(loadings = matrix)}
(both loadings and factor_cor_mat can be matrices or data.frames)
\item For GM: \code{list(means = matrix, covariances = array, proportions = numeric, z = matrix)}
\item For IRT: Not yet implemented
\item For CDM: Not yet implemented
}
}}

\item{chat_session}{Optional. A chat_session object created with \code{\link{chat_session}}
for token-efficient multi-analysis workflows (default = NULL).}

\item{analysis_type}{Character. Type of analysis ("fa", "gm", "irt", "cdm"). Required when using
structured list without chat_session. Automatically inferred from chat_session if provided (default = NULL).}

\item{llm_provider}{Character. LLM provider (e.g., "openai", "anthropic", "ollama", "gemini").
Required when chat_session is NULL. Top-level convenience parameter (default = NULL).}

\item{llm_model}{Character. Specific model to use (e.g., "gpt-4o-mini", "claude-3-5-sonnet-20241022", "gemma2:9b").
If NULL, uses provider default. Top-level convenience parameter (default = NULL).}

\item{llm_args}{List or llm_args object. LLM configuration settings. Can be created with
\code{\link{llm_args}} or passed as a plain list. Contains: system_prompt, params, word_limit,
interpretation_guidelines, additional_info, echo. If llm_provider/llm_model are provided at top-level,
they override values in llm_args (default = NULL).}

\item{interpretation_args}{List or interpretation_args object. Model-specific interpretation configuration.
Can be created with \code{\link{interpretation_args}} or passed as a plain list. Contents vary by model type.
For FA: cutoff, n_emergency, hide_low_loadings, sort_loadings (default = NULL).}

\item{output_args}{List or output_args object. Output configuration settings. Can be created with
\code{\link{output_args}} or passed as a plain list. Contains: format, heading_level,
suppress_heading, max_line_length, silent (default = NULL).}

\item{...}{Additional arguments passed to model-specific methods. Model-specific methods may require:
\itemize{
\item \code{variable_info}: Data frame with 'variable' and 'description' columns (required for FA)
}}
}
\value{
Model-specific interpretation object:
\itemize{
\item FA: \code{fa_interpretation} (see \code{\link{interpret}})
\item GM: \code{gm_interpretation} with cluster profiles and interpretations
\item Future: \code{irt_interpretation}, \code{cdm_interpretation}
}
}
\description{
Unified interface for interpreting psychometric analysis results with LLMs.
Supports fitted model objects, structured lists, and persistent chat sessions
for efficient multi-analysis workflows.
}
\details{
All arguments are named to prevent positional confusion. The function detects which pattern
you're using based on which arguments are provided.
\subsection{Usage Patterns}{

\strong{Pattern 1: Fitted Model Object}

Automatically extracts model components from fitted objects.

\preformatted{
interpret(
  fit_results = fa_model,
  variable_info = var_info,
  llm_provider = "ollama",
  llm_model = "gpt-oss:20b-cloud"
)
}

\strong{Pattern 2: Structured List}

For custom data structures or manual extraction.

For FA, provide loadings (required) and optionally factor_cor_mat:

\preformatted{
interpret(
  fit_results = list(
    loadings = loadings_matrix,
    factor_cor_mat = factor_cor_mat
  ),
  variable_info = var_info,
  analysis_type = "fa"
)
}

\strong{Pattern 3: Chat Session (Token-Efficient)}

Reuse chat session across analyses to save tokens.

\preformatted{
chat <- chat_session(analysis_type = "fa", llm_provider = "ollama", llm_model = "gpt-oss:20b-cloud")
result1 <- interpret(chat_session = chat, fit_results = model1, variable_info = var_info1)
result2 <- interpret(chat_session = chat, fit_results = model2, variable_info = var_info2)
}

\strong{Pattern 4: Configuration Objects (Advanced)}

Use configuration objects for reusable settings and cleaner code.

\preformatted{
# Create configuration objects
interp_config <- interpretation_args(analysis_type = "fa", cutoff = 0.4, n_emergency = 2)
llm_config <- llm_args(word_limit = 100, additional_info = "Study context")
output_config <- output_args(format = "markdown", silent = 1)

# Use in interpret() call
interpret(
  fit_results = fa_model,
  variable_info = var_info,
  interpretation_args = interp_config,
  llm_args = llm_config,
  output_args = output_config,
  llm_provider = "ollama",
  llm_model = "gpt-oss:20b-cloud"
)

# Or mix config objects with direct parameters (direct parameters override)
interpret(
  fit_results = fa_model,
  variable_info = var_info,
  interpretation_args = interp_config,
  llm_provider = "ollama",
  llm_model = "gpt-oss:20b-cloud",
  word_limit = 150  # Overrides llm_config if it were provided
)
}
}

\subsection{Discovering Available Parameters}{

To see all available parameters with their defaults, types, and valid ranges,
use \code{\link{show_interpret_args}}:

\preformatted{
# Show common parameters (llm_args + output_args)
show_interpret_args()

# Show all parameters for Factor Analysis
show_interpret_args("fa")

# Show all parameters for Gaussian Mixture Models
show_interpret_args("gm")
}
}

\subsection{Supported Model Types}{
\itemize{
\item \strong{fa} (Factor Analysis): psych::fa(), psych::principal(), lavaan::cfa/efa(), mirt::mirt()
\item \strong{gm} (Gaussian Mixture): mclust::Mclust()
\item \strong{irt} (Item Response Theory): Not yet implemented
\item \strong{cdm} (Cognitive Diagnosis Models): Not yet implemented
}
}
}
\note{
While only \code{llm_provider} and \code{llm_model} are exposed as top-level parameters for convenience,
the dual interface pattern allows any argument to be passed either directly or through the respective
configuration object (\code{llm_args}, \code{interpretation_args}, \code{output_args}). This keeps the primary
signature clean while maintaining flexibility
}
\examples{
\dontrun{
library(psych)
fa_model <- fa(mtcars[,1:4], nfactors = 2, rotate = "oblimin")

var_info <- data.frame(
  variable = c("mpg", "cyl", "disp", "hp"),
  description = c("Miles per gallon", "Cylinders", "Displacement", "Horsepower")
)

# Pattern 1: Fitted model
result1 <- interpret(
  fit_results = fa_model,
  variable_info = var_info,
  llm_provider = "ollama",
  llm_model = "gpt-oss:20b-cloud"
)

# Pattern 2: Structured list
# Extract loadings from fitted model
loadings <- as.data.frame(unclass(fa_model$loadings))

# Option A: Loadings only (orthogonal rotation)
result2a <- interpret(
  fit_results = list(loadings = loadings),
  variable_info = var_info,
  analysis_type = "fa",
  llm_provider = "ollama",
  llm_model = "gpt-oss:20b-cloud"
)

# Option B: Loadings + factor correlations (oblique rotation)
result2b <- interpret(
  fit_results = list(
    loadings = loadings,
    factor_cor_mat = fa_model$Phi
  ),
  variable_info = var_info,
  analysis_type = "fa",
  llm_provider = "ollama",
  llm_model = "gpt-oss:20b-cloud"
)

# Pattern 3: Chat session (token-efficient for multiple analyses)
chat <- chat_session(analysis_type = "fa", llm_provider = "ollama", llm_model = "gpt-oss:20b-cloud")
result3a <- interpret(chat_session = chat, fit_results = fa_model, variable_info = var_info)
result3b <- interpret(chat_session = chat, fit_results = fa_model2, variable_info = var_info2)
print(chat)  # Check token usage
}
}
\seealso{
\code{\link{interpret}}, \code{\link{chat_session}}, \code{\link{show_interpret_args}}
}
