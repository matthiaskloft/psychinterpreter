% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/chat_fa.R
\name{chat_fa}
\alias{chat_fa}
\title{Create Factor Analysis Chat Session}
\usage{
chat_fa(
  provider,
  model = NULL,
  system_prompt = NULL,
  params = NULL,
  echo = "none"
)
}
\arguments{
\item{provider}{Character. LLM provider (e.g., "anthropic", "openai", "ollama")}

\item{model}{Character. Model name (e.g., "claude-haiku-4-5-20251001")}

\item{system_prompt}{Character or NULL. Optional custom system prompt text to override the package default
psychometric system prompt. Use this to provide institution- or project-specific framing for the LLM
(e.g., preferred terminology, audience level, or reporting conventions). If NULL the internal default
system prompt is used (default = NULL).}

\item{params}{List. ellmer parameters (temperature, etc.). Default uses ellmer::params()}

\item{echo}{Character. Echo level ("none", "output", "all"). Default is "none"}
}
\value{
A chat_fa object containing the persistent chat session
}
\description{
Creates a persistent chat session for factor analysis interpretation that can be reused
across multiple analyses to avoid repeating the system prompt and reduce
token costs.
}
\details{
The chat_fa object stores:
\itemize{
\item A persistent ellmer chat session with the factor analysis system prompt already loaded
\item Provider and model information
\item Token usage tracking (cumulative input/output tokens across all interpretations)
\item Session metadata (number of interpretations run, creation timestamp)
}

This allows for efficient reuse across multiple factor analysis interpretations without
resending the system prompt each time.

\strong{Token Tracking Note:}
The package tracks per-interpretation token counts using roles returned by
\code{ellmer::chat$get_tokens()} (user/assistant). Some providers or the \code{ellmer}
wrapper do not consistently expose \code{system}/\code{system_prompt} token counts (this
appears to be an upstream limitation/bug). To avoid incorrect cumulative
accounting (double-counting or negative accumulation), \code{chat_fa} intentionally
does NOT include \code{system_prompt} tokens in the package-level cumulative
counters (\code{total_input_tokens} / \code{total_output_tokens}).

If you require a provider-specific view that includes system prompt tokens,
call \code{chat$chat$get_tokens(include_system_prompt = TRUE)} directly â€” but note
that results may vary across providers. Use \code{results$run_tokens} to access
per-interpretation token counts produced by \code{interpret_fa()}.
}
\examples{
\dontrun{
# Create a persistent chat session
chat <- chat_fa("anthropic", "claude-haiku-4-5-20251001")

# Use with multiple analyses
result1 <- interpret_fa(loadings1, var_info1, chat_session = chat)
result2 <- interpret_fa(loadings2, var_info2, chat_session = chat)

# Check token usage
print(chat)
}

}
