% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fa_interpret.R
\name{interpret_fa}
\alias{interpret_fa}
\title{Interpret Exploratory Factor Analysis Results}
\usage{
interpret_fa(
  loadings,
  variable_info,
  factor_cor_mat = NULL,
  chat_session = NULL,
  llm_provider = NULL,
  llm_model = NULL,
  params = NULL,
  cutoff = 0.3,
  n_emergency = 2,
  hide_low_loadings = FALSE,
  sort_loadings = TRUE,
  system_prompt = NULL,
  interpretation_guidelines = NULL,
  additional_info = NULL,
  word_limit = 150,
  output_format = "cli",
  heading_level = 1,
  suppress_heading = FALSE,
  max_line_length = 80,
  silent = 0,
  echo = "none"
)
}
\arguments{
\item{loadings}{A dataframe or matrix of factor loadings (variables x factors)}

\item{variable_info}{A dataframe with at least two columns:
\itemize{
\item variable: variable names matching row names in loadings
\item description: labels or descriptions of the variables
}}

\item{factor_cor_mat}{Matrix or data.frame. Optional factor correlation matrix for oblique rotations.
Should have factor names as row and column names matching the loadings factor columns. Used to inform
interpretation of factor relationships and enhance understanding of discriminant validity. Higher
correlations suggest related constructs, while lower correlations indicate distinct factors.
If NULL, factors are assumed to be orthogonal (default = NULL)}

\item{chat_session}{A chat_fa object for reusing existing chat sessions. If provided, uses the existing
chat session (avoiding resending the system prompt and reducing token costs). If NULL, creates a new
chat session. Useful for processing multiple analyses efficiently (default = NULL)}

\item{llm_provider}{Character. Which LLM provider to use.
Any provider supported by ellmer::chat() (e.g., "openai", "anthropic", "ollama", etc.)
See ellmer documentation for the complete list of supported providers.}

\item{llm_model}{Character. Specific model to use (e.g., "gpt-4o-mini", "claude-3-5-sonnet-20241022", "gemma2:9b")}

\item{params}{Parameters for the LLM created using ellmer::params() function (e.g., params(temperature = 0.7, seed = 42)).
Provides a provider-agnostic interface for setting model parameters like temperature, seed, max_tokens, etc.
If NULL, uses provider defaults. See ellmer::params() documentation for supported parameters.}

\item{cutoff}{Numeric. Minimum loading value to consider (default = 0.3)}

\item{n_emergency}{Integer. When a factor has no loadings above the cutoff, use the
top N highest loadings (even if below cutoff) for interpretation. If set to 0,
factors with no significant loadings are labeled as "undefined" and assigned NA
interpretations (default = 2)}

\item{hide_low_loadings}{Logical. If TRUE, only variables with loadings at or above
the cutoff are included in the data sent to the LLM. If FALSE, all loadings are
included regardless of magnitude (default = FALSE)}

\item{sort_loadings}{Logical. Sort variables by loading strength within factors (default = TRUE)}

\item{system_prompt}{Character or NULL. Optional custom system prompt text to override the package default
psychometric system prompt. Use this to provide institution- or project-specific framing for the LLM
(e.g., preferred terminology, audience level, or reporting conventions). If NULL the internal default
system prompt is used (default = NULL). This will be ignored if chat_session is used.}

\item{interpretation_guidelines}{Character or NULL. Optional custom interpretation guidelines for the LMM that override
the package default guidelines. If NULL, built-in interpretation
guidelines are applied (default = NULL).}

\item{additional_info}{Character. Optional additional context for the LLM, such as theoretical background,
research area information, or domain-specific knowledge to inform factor interpretation (default = NULL)}

\item{word_limit}{Integer. Maximum number of words for LLM interpretations (default = 150)}

\item{output_format}{Character. Output format for the report: "cli" or "markdown" (default = "cli")}

\item{heading_level}{Integer. Starting heading level for markdown output (default = 1). Used when output_format = "markdown"}

\item{suppress_heading}{Logical. If TRUE, suppresses the main "Exploratory Factor Analysis Interpretation"
heading, allowing for better integration into existing documents that already have appropriate headings
above the analysis output (default = FALSE)}

\item{max_line_length}{Integer. Maximum line length for console output text wrapping (default = 80)}

\item{silent}{Integer or logical. Controls output verbosity:
\itemize{
\item 0 or FALSE: Show report and all messages (default)
\item 1: Show messages only, suppress report
\item 2 or TRUE: Completely silent, suppress all output
For backward compatibility, logical values are accepted and converted to integers.
}}

\item{echo}{Character. Controls what is echoed during LLM interaction. One of "none" (no output),
"output" (show only LLM responses), or "all" (show prompts and responses). Passed directly
to the ellmer chat function for debugging and transparency (default = "none")}
}
\value{
A fa_interpretation object (S3 class) containing:
\describe{
\item{factor_summaries}{List with detailed analysis for each factor, including variables,
loadings, significance status, and variance_explained}
\item{suggested_names}{Named list of LLM-generated factor names}
\item{loading_matrix}{Formatted loading matrix with small loadings optionally suppressed}
\item{report}{Complete formatted text/markdown report including factor correlations (when provided)}
\item{llm_info}{List containing provider name and model used for interpretation}
\item{chat}{Complete chat_fa object with full conversation history}
\item{cross_loadings}{Data frame of variables loading significantly on multiple factors}
\item{no_loadings}{Data frame of variables with no loadings above cutoff threshold}
\item{elapsed_time}{Total analysis time as difftime object}
\item{factor_cor_mat}{Factor correlation matrix (when provided)}
\item{cutoff}{Numeric cutoff value used for determining significant loadings}
}
}
\description{
This function uses Large Language Models (LLMs) to automatically interpret FA results by
analyzing factor loadings and variable descriptions. It generates suggested factor names,
detailed interpretations, and identifies cross-loadings. Variance explained is automatically
calculated from the loadings (sum of squared loadings / number of variables).
}
\details{
This function uses advanced processing to minimize LLM API calls and costs. Instead of individual
factor interpretation calls, it processes all factors simultaneously in a single comprehensive prompt
with structured sections for optimal LLM comprehension.

\strong{Key Features:}
\itemize{
\item \strong{Structured LLM Prompting}: Uses organized sections (Factor Naming, Factor Interpretation,
Factor Relationships, Output Requirements) for consistent, high-quality results
\item \strong{Factor Correlation Integration}: When factor_cor_mat is provided, incorporates factor
relationships into interpretations for enhanced discriminant validity assessment
\item \strong{Optimized Word Targeting}: Targets 80\%-100\% of word limit for comprehensive interpretations
\item \strong{Emergency Rule}: Factors with no loadings above cutoff use top N highest loadings
\item \strong{Cross-loading Detection}: Identifies variables loading on multiple factors
\item \strong{Batch Processing}: Single API call processes all factors simultaneously
}

\strong{Technical Details:}
\itemize{
\item Variance explained calculated as sum of squared loadings divided by number of variables
\item Supports both orthogonal (factor_cor_mat = NULL) and oblique rotation results
\item Uses compact vector format for efficient token usage in LLM prompts
\item JSON output parsing with fallback extraction methods for robustness
}
}
\note{
This function requires:
\itemize{
\item Internet connectivity for LLM API calls
\item Valid API credentials for the chosen LLM provider (set via environment variables)
\item The ellmer package for LLM communication
}
}
\examples{
\dontrun{
# Set up API credentials first
Sys.setenv(OPENAI_API_KEY = "your-api-key-here")

# Basic interpretation with OpenAI
interpretation <- interpret_fa(
  loadings = fa_results$loadings,
  variable_info = var_info,
  llm_provider = "openai",
  llm_model = "gpt-4o-mini"
)

# Using persistent chat session for multiple analyses (efficient)
chat <- chat_fa("anthropic", "claude-haiku-4-5-20251001")
interpretation1 <- interpret_fa(
  loadings = fa_results1$loadings,
  variable_info = var_info1,
  chat_session = chat
)
print(chat)  # Check token usage

# Advanced usage with factor correlations
interpretation_advanced <- interpret_fa(
  loadings = fa_results$loadings,
  variable_info = var_info,
  llm_provider = "ollama",
  llm_model = "llama3.1:8b",
  factor_cor_mat = fa_results$Phi,
  cutoff = 0.4,
  word_limit = 120,
  params = params(temperature = 0.3, seed = 123),
  output_format = "markdown"
)
}
}
