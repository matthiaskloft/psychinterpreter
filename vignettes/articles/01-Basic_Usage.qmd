---
title: "psychinterpreter: Getting Started"
subtitle: "A Basic Usage Example with Big Five Personality Data"
author: "Matthias Kloft"
date: "`r Sys.Date()`"
format: 
  html:
    toc: true
    toc-depth: 3
    code-fold: show
    source: repo
    code-tools: true
    df-print: paged
execute:
  warning: true
  message: true
---

## Overview

This document demonstrates the functionality of the **psychinterpreter** package using the Big Five personality data from the `psych` package. The Big Five Inventory (BFI) is a widely-used personality assessment that measures five major dimensions of personality: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism.

We'll test all major features of psychinterpreter including:

- Basic factor analysis interpretation
- Persistent chat sessions for efficient LLM usage
- Export functionality across multiple formats
- Advanced interpretation features

## Setup and Data Preparation

```{r setup}
packages <- c(
  "psychinterpreter",
  "ellmer",
  "ollamar",
  "psych",
  "lavaan",
  "mirt",
  "dplyr",
  "here",
  "pander"
)
# Load required packages using pacman
if (!require("pacman")) install.packages("pacman")
pacman::p_load(char = packages)

# Load the BFI dataset and dictionary
data("bfi")
data("bfi.dictionary")
```

# Preparing the BFI Data

Exclude demographic variables for factor analysis

```{r}
bfi <- bfi[,1:25]
bfi.dictionary <- bfi.dictionary[1:25, ]
```

Show the first few rows of the BFI data:
```{r data-exploration}
head(bfi,10)
```

Display the BFI dictionary showing item descriptions:
```{r}
bfi.dictionary |> select(Item) |> as.data.frame()
```


# Factor Analysis

## Determining Number of Factors

```{r parallel-analysis}
# Parallel analysis to determine optimal number of factors
pa_result <- fa.parallel(
  bfi,
  fa = "fa",
  n.iter = 100,
  use = "pairwise.complete.obs",
  main = "Parallel Analysis: BFI Data"
)
```

## Exploratory Factor Analysis Using 6 Factors

Perform FA with 6 factors identified by the parallel analysis
```{r fa-analysis}
fa_result <- fa(
  bfi,
  nfactors = pa_result$nfact,
  rotate = "oblimin",
  fm = "ml"
)
summary(fa_result)
```

```{r}
# Extract loading matrix for interpretation
loadings_matrix <- unclass(fa_result$loadings) |> as.data.frame()
round(loadings_matrix, 2)

# Extract factor correlation matrix
factor_cor_mat <- fa_result$Phi |> as.data.frame()
round(factor_cor_mat, 2)
```


## Chat Setup
Set up persistent chat session. If you use Ollama, make sure to start the server befor running this.

```{r test-chat-sessions}
# test the connection to the Ollama server
#ollamar::test_connection()

# optionally pull the model from ollama
#ollamar::pull("gpt-oss:20b-cloud")

chat <- chat_fa(
  provider = "ollama",
  model = "gpt-oss:20b-cloud",
  # set temperature and seed for reproducibility
  params = params(temperature = 0, seed = 42)
)

print(chat)
```

## Interpreting Factor Analysis Results

Create variable information dataframe from BFI dictionary
```{r test-basic-interpretation}
variable_info <- data.frame(
  variable = rownames(bfi.dictionary),
  description = as.character(bfi.dictionary$Item),
  stringsAsFactors = FALSE
)

head(variable_info, 10)
```

We now generate factor interpretations using the specified LLM:
```{r}
#| output: asis

word_limit <-  50
result <- interpret_fa(
  chat_session = chat,
  loadings = loadings_matrix,
  variable_info = variable_info,
  cutoff = 0.23,
  n_emergency = 2,
  additional_info = NULL,
  factor_cor_mat = factor_cor_mat,
  sort_loadings = TRUE,
  word_limit =  word_limit,
  max_line_length = 80,
  silent = FALSE,
  echo = "none",
  output_format = "markdown",
  heading_level = 2,
  suppress_heading = TRUE
)
```

Let's check the global token usage tracked by the chat:
```{r usage1}
print(chat)
```

We can also examine the prompt:
```{r prompt}
cat(
  "System Prompt: \n\n",
  result$system_prompt, "\n",
  "Main Prompt: \n\n",
    result$prompt)
```



Plot the loading matrix with factor names:
```{r}
plot(result,
     cutoff = 0.3)
```

## Export Functionality

We can export the results to multiple formats

```{r exports, eval=FALSE}
# TXT export
export_interpretation(result,
                      format = "txt",
                      file = here("exports", "bfi_analysis"))
# MD export
export_interpretation(result,
                      format = "md",
                      file = here("exports", "bfi_analysis"))
```

## Generic Methods for 'psych', 'lavaan', and 'mirt' Objects

### psych Package
```{r method-psych}
#| output: asis

result_psych <- interpret(
  fa_result,
  variable_info = variable_info,
  cutoff = 0.3,
  chat_session = chat,
  word_limit =  word_limit,
  heading_level = 3,
  suppress_heading = TRUE,
  output_format = "markdown",
  silent = TRUE
)
result_psych$suggested_names
```

### lavaan Package
```{r method-lavaan-efa}
#| output: asis

# Specify a CFA model for BFI data
fa_lavaan <- lavaan::efa(bfi, nfactors = pa_result$nfact, rotation = "oblimin")
# interpret lavaan model
result_lavaan_efa <- interpret(
  fa_lavaan,
  variable_info = variable_info,
  cutoff = 0.3,
  chat_session = chat,
  word_limit =  word_limit,
  heading_level = 3,
  suppress_heading = TRUE,
  output_format = "markdown",
  silent = TRUE
)
result_lavaan_efa$suggested_names
```

Also works with lavaa::cfa():
```{r method-lavaan-cfa}
#| output: asis

# specify and fit a CFA model
cfa_model <- "
  O =~ O1 + O2 + O3 + O4 + O5
  C =~ C1 + C2 + C3 + C4 + C5
  E =~ E1 + E2 + E3 + E4 + E5
  A =~ A1 + A2 + A3 + A4 + A5
  N =~ N1 + N2 + N3 + N4 + N5
  "
cfa_fit <- lavaan::cfa(cfa_model, data = bfi, std.lv = TRUE)

# interpret lavaan cfa model
result_lavaan_cfa <- interpret(
  cfa_fit,
  variable_info = variable_info,
  cutoff = 0.3,
  chat_session = chat,
  word_limit =  word_limit,
  output_format = "markdown",
  heading_level = 3,
  suppress_heading = TRUE,
  silent = TRUE
)

result_lavaan_cfa$suggested_names
```

### mirt Package
```{r method-mirt}
#| output: asis

# Fit a unidimensional IRT model
suppressMessages(
  irt_model <- mirt::mirt(
    data = bfi,
    model =  pa_result$nfact,
    itemtype = "graded",
    method = "MHRM"
  )
)

# interpret mirt model
result_mirt <- interpret(
  irt_model,
  variable_info = variable_info,
  cutoff = 0.3,
  chat_session = chat,
  word_limit =  word_limit,
  heading_level = 3,
  suppress_heading = TRUE,
  output_format = "markdown",
  silent = TRUE
)

result_mirt$ suggested_names
```

# Cluster Analysis

Coming soon ...

## Total Usage of the Chat Session

We can again check the total global token usage:
```{r usage2}
print(chat)
```


# Session Info
```{r}
pander::pander(sessionInfo())
```

